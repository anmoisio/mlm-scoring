
Currently Loaded Modules:
  1) CUDA/9.0.176   2) cuDNN/7-CUDA-9.0.176   3) anaconda/2021-03-tf2

 

nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2017 NVIDIA Corporation
Built on Fri_Sep__1_21:08:03_CDT_2017
Cuda compilation tools, release 9.0, V9.0.176
Package            Version
------------------ -------------------
certifi            2021.5.30
charset-normalizer 2.0.4
graphviz           0.8.4
idna               3.2
mxnet-cu110        1.8.0.post0
numpy              1.21.1
pip                21.2.2
requests           2.26.0
setuptools         52.0.0.post20210125
urllib3            1.26.6
wheel              0.36.2
Obtaining file:///scratch/work/moisioa3/conv_lm/mlm-scoring
Collecting gluonnlp~=0.8.3
  Using cached gluonnlp-0.8.3-py3-none-any.whl
Collecting regex
  Using cached regex-2021.7.6-cp39-cp39-manylinux2014_x86_64.whl (733 kB)
Collecting sacrebleu
  Using cached sacrebleu-1.5.1-py3-none-any.whl (54 kB)
Collecting mosestokenizer
  Using cached mosestokenizer-1.1.0-py3-none-any.whl
Collecting transformers~=3.3.1
  Using cached transformers-3.3.1-py3-none-any.whl (1.1 MB)
Requirement already satisfied: numpy in /home/moisioa3/.conda/envs/mlm-scoring5/lib/python3.9/site-packages (from gluonnlp~=0.8.3->mlm==0.1) (1.21.1)
Collecting packaging
  Using cached packaging-21.0-py3-none-any.whl (40 kB)
Collecting filelock
  Using cached filelock-3.0.12-py3-none-any.whl (7.6 kB)
Collecting sacremoses
  Using cached sacremoses-0.0.45-py3-none-any.whl (895 kB)
Collecting tokenizers==0.8.1.rc2
  Using cached tokenizers-0.8.1rc2.tar.gz (97 kB)
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
    Preparing wheel metadata: started
    Preparing wheel metadata: finished with status 'done'
Requirement already satisfied: requests in /home/moisioa3/.conda/envs/mlm-scoring5/lib/python3.9/site-packages (from transformers~=3.3.1->mlm==0.1) (2.26.0)
Collecting tqdm>=4.27
  Using cached tqdm-4.62.0-py2.py3-none-any.whl (76 kB)
Collecting sentencepiece!=0.1.92
  Using cached sentencepiece-0.1.96-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)
Collecting openfile
  Using cached openfile-0.0.7-py3-none-any.whl (2.4 kB)
Collecting docopt
  Using cached docopt-0.6.2-py2.py3-none-any.whl
Collecting uctools
  Using cached uctools-1.3.0-py3-none-any.whl
Collecting toolwrapper
  Using cached toolwrapper-2.1.0-py3-none-any.whl
Collecting pyparsing>=2.0.2
  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/moisioa3/.conda/envs/mlm-scoring5/lib/python3.9/site-packages (from requests->transformers~=3.3.1->mlm==0.1) (1.26.6)
Requirement already satisfied: certifi>=2017.4.17 in /home/moisioa3/.conda/envs/mlm-scoring5/lib/python3.9/site-packages (from requests->transformers~=3.3.1->mlm==0.1) (2021.5.30)
Requirement already satisfied: idna<4,>=2.5 in /home/moisioa3/.conda/envs/mlm-scoring5/lib/python3.9/site-packages (from requests->transformers~=3.3.1->mlm==0.1) (3.2)
Requirement already satisfied: charset-normalizer~=2.0.0 in /home/moisioa3/.conda/envs/mlm-scoring5/lib/python3.9/site-packages (from requests->transformers~=3.3.1->mlm==0.1) (2.0.4)
Collecting portalocker==2.0.0
  Using cached portalocker-2.0.0-py2.py3-none-any.whl (11 kB)
Collecting joblib
  Using cached joblib-1.0.1-py3-none-any.whl (303 kB)
Collecting six
  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)
Collecting click
  Using cached click-8.0.1-py3-none-any.whl (97 kB)
Building wheels for collected packages: tokenizers
  Building wheel for tokenizers (PEP 517): started
  Building wheel for tokenizers (PEP 517): finished with status 'error'
  ERROR: Command errored out with exit status 1:
   command: /home/moisioa3/.conda/envs/mlm-scoring5/bin/python /home/moisioa3/.conda/envs/mlm-scoring5/lib/python3.9/site-packages/pip/_vendor/pep517/in_process/_in_process.py build_wheel /tmp/tmpi9r3t3sx
       cwd: /tmp/pip-install-pn9u5ixc/tokenizers_c2f4526cbec64e8ab961337a56ac02c9
  Complete output (48 lines):
  /tmp/pip-build-env-c24trno5/overlay/lib/python3.9/site-packages/setuptools/dist.py:484: UserWarning: Normalizing '0.8.1.rc2' to '0.8.1rc2'
    warnings.warn(tmpl.format(**locals()))
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib
  creating build/lib/tokenizers
  copying tokenizers/__init__.py -> build/lib/tokenizers
  creating build/lib/tokenizers/models
  copying tokenizers/models/__init__.py -> build/lib/tokenizers/models
  creating build/lib/tokenizers/decoders
  copying tokenizers/decoders/__init__.py -> build/lib/tokenizers/decoders
  creating build/lib/tokenizers/normalizers
  copying tokenizers/normalizers/__init__.py -> build/lib/tokenizers/normalizers
  creating build/lib/tokenizers/pre_tokenizers
  copying tokenizers/pre_tokenizers/__init__.py -> build/lib/tokenizers/pre_tokenizers
  creating build/lib/tokenizers/processors
  copying tokenizers/processors/__init__.py -> build/lib/tokenizers/processors
  creating build/lib/tokenizers/trainers
  copying tokenizers/trainers/__init__.py -> build/lib/tokenizers/trainers
  creating build/lib/tokenizers/implementations
  copying tokenizers/implementations/bert_wordpiece.py -> build/lib/tokenizers/implementations
  copying tokenizers/implementations/char_level_bpe.py -> build/lib/tokenizers/implementations
  copying tokenizers/implementations/base_tokenizer.py -> build/lib/tokenizers/implementations
  copying tokenizers/implementations/__init__.py -> build/lib/tokenizers/implementations
  copying tokenizers/implementations/byte_level_bpe.py -> build/lib/tokenizers/implementations
  copying tokenizers/implementations/sentencepiece_bpe.py -> build/lib/tokenizers/implementations
  copying tokenizers/__init__.pyi -> build/lib/tokenizers
  copying tokenizers/models/__init__.pyi -> build/lib/tokenizers/models
  copying tokenizers/decoders/__init__.pyi -> build/lib/tokenizers/decoders
  copying tokenizers/normalizers/__init__.pyi -> build/lib/tokenizers/normalizers
  copying tokenizers/pre_tokenizers/__init__.pyi -> build/lib/tokenizers/pre_tokenizers
  copying tokenizers/processors/__init__.pyi -> build/lib/tokenizers/processors
  copying tokenizers/trainers/__init__.pyi -> build/lib/tokenizers/trainers
  running build_ext
  running build_rust
  error: can't find Rust compiler
  
  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.
  
  To update pip, run:
  
      pip install --upgrade pip
  
  and then retry package installation.
  
  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.
  ----------------------------------------
  ERROR: Failed building wheel for tokenizers
Failed to build tokenizers
ERROR: Could not build wheels for tokenizers which use PEP 517 and cannot be installed directly
Package            Version
------------------ -------------------
certifi            2021.5.30
charset-normalizer 2.0.4
graphviz           0.8.4
idna               3.2
mxnet-cu110        1.8.0.post0
numpy              1.21.1
pip                21.2.2
requests           2.26.0
setuptools         52.0.0.post20210125
urllib3            1.26.6
wheel              0.36.2
