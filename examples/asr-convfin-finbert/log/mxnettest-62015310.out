
Currently Loaded Modules:
  1) CUDA/9.0.176   2) cuDNN/7-CUDA-9.0.176   3) miniconda/4.9.2

 

nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2017 NVIDIA Corporation
Built on Fri_Sep__1_21:08:03_CDT_2017
Cuda compilation tools, release 9.0, V9.0.176
Package    Version
---------- -------------------
certifi    2021.5.30
pip        21.1.3
setuptools 52.0.0.post20210125
wheel      0.36.2
Obtaining file:///scratch/work/moisioa3/conv_lm/mlm-scoring
Collecting gluonnlp~=0.8.3
  Using cached gluonnlp-0.8.3.tar.gz (236 kB)
Collecting regex
  Downloading regex-2021.7.6-cp39-cp39-manylinux2014_x86_64.whl (733 kB)
Collecting sacrebleu
  Using cached sacrebleu-1.5.1-py3-none-any.whl (54 kB)
Collecting mosestokenizer
  Using cached mosestokenizer-1.1.0.tar.gz (37 kB)
Collecting transformers~=3.3.1
  Using cached transformers-3.3.1-py3-none-any.whl (1.1 MB)
Collecting numpy
  Downloading numpy-1.21.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.8 MB)
Collecting tqdm>=4.27
  Downloading tqdm-4.62.0-py2.py3-none-any.whl (76 kB)
Collecting sentencepiece!=0.1.92
  Downloading sentencepiece-0.1.96-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)
Collecting packaging
  Using cached packaging-21.0-py3-none-any.whl (40 kB)
Collecting sacremoses
  Using cached sacremoses-0.0.45-py3-none-any.whl (895 kB)
Collecting filelock
  Using cached filelock-3.0.12-py3-none-any.whl (7.6 kB)
Collecting tokenizers==0.8.1.rc2
  Downloading tokenizers-0.8.1rc2.tar.gz (97 kB)
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
    Preparing wheel metadata: started
    Preparing wheel metadata: finished with status 'done'
Collecting requests
  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)
Collecting docopt
  Using cached docopt-0.6.2.tar.gz (25 kB)
Collecting openfile
  Using cached openfile-0.0.7-py3-none-any.whl (2.4 kB)
Collecting uctools
  Using cached uctools-1.3.0.tar.gz (4.6 kB)
Collecting toolwrapper
  Using cached toolwrapper-2.1.0.tar.gz (3.2 kB)
Collecting pyparsing>=2.0.2
  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)
Collecting urllib3<1.27,>=1.21.1
  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)
Collecting charset-normalizer~=2.0.0
  Downloading charset_normalizer-2.0.4-py3-none-any.whl (36 kB)
Requirement already satisfied: certifi>=2017.4.17 in /home/moisioa3/.conda/envs/mlm-scoring5/lib/python3.9/site-packages (from requests->transformers~=3.3.1->mlm==0.1) (2021.5.30)
Collecting idna<4,>=2.5
  Downloading idna-3.2-py3-none-any.whl (59 kB)
Collecting portalocker==2.0.0
  Using cached portalocker-2.0.0-py2.py3-none-any.whl (11 kB)
Collecting joblib
  Using cached joblib-1.0.1-py3-none-any.whl (303 kB)
Collecting click
  Using cached click-8.0.1-py3-none-any.whl (97 kB)
Collecting six
  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)
Building wheels for collected packages: gluonnlp, tokenizers, mosestokenizer, docopt, toolwrapper, uctools
  Building wheel for gluonnlp (setup.py): started
  Building wheel for gluonnlp (setup.py): finished with status 'done'
  Created wheel for gluonnlp: filename=gluonnlp-0.8.3-py3-none-any.whl size=293539 sha256=9bf91190177e69a6ba320d015d7fc455d858e39c456385cba89533daa120abe4
  Stored in directory: /home/moisioa3/.cache/pip/wheels/14/f5/30/c473185b177f1b61692bc648e2813243d04fb2fd5d3f2ac9a9
  Building wheel for tokenizers (PEP 517): started
  Building wheel for tokenizers (PEP 517): finished with status 'error'
  ERROR: Command errored out with exit status 1:
   command: /home/moisioa3/.conda/envs/mlm-scoring5/bin/python /home/moisioa3/.conda/envs/mlm-scoring5/lib/python3.9/site-packages/pip/_vendor/pep517/in_process/_in_process.py build_wheel /tmp/tmpwxrr6gi2
       cwd: /tmp/pip-install-kwilhmjx/tokenizers_d0ced63539fe49a89c32e56923a29370
  Complete output (48 lines):
  /tmp/pip-build-env-37ow9adt/overlay/lib/python3.9/site-packages/setuptools/dist.py:484: UserWarning: Normalizing '0.8.1.rc2' to '0.8.1rc2'
    warnings.warn(tmpl.format(**locals()))
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib
  creating build/lib/tokenizers
  copying tokenizers/__init__.py -> build/lib/tokenizers
  creating build/lib/tokenizers/models
  copying tokenizers/models/__init__.py -> build/lib/tokenizers/models
  creating build/lib/tokenizers/decoders
  copying tokenizers/decoders/__init__.py -> build/lib/tokenizers/decoders
  creating build/lib/tokenizers/normalizers
  copying tokenizers/normalizers/__init__.py -> build/lib/tokenizers/normalizers
  creating build/lib/tokenizers/pre_tokenizers
  copying tokenizers/pre_tokenizers/__init__.py -> build/lib/tokenizers/pre_tokenizers
  creating build/lib/tokenizers/processors
  copying tokenizers/processors/__init__.py -> build/lib/tokenizers/processors
  creating build/lib/tokenizers/trainers
  copying tokenizers/trainers/__init__.py -> build/lib/tokenizers/trainers
  creating build/lib/tokenizers/implementations
  copying tokenizers/implementations/base_tokenizer.py -> build/lib/tokenizers/implementations
  copying tokenizers/implementations/bert_wordpiece.py -> build/lib/tokenizers/implementations
  copying tokenizers/implementations/char_level_bpe.py -> build/lib/tokenizers/implementations
  copying tokenizers/implementations/sentencepiece_bpe.py -> build/lib/tokenizers/implementations
  copying tokenizers/implementations/__init__.py -> build/lib/tokenizers/implementations
  copying tokenizers/implementations/byte_level_bpe.py -> build/lib/tokenizers/implementations
  copying tokenizers/__init__.pyi -> build/lib/tokenizers
  copying tokenizers/models/__init__.pyi -> build/lib/tokenizers/models
  copying tokenizers/decoders/__init__.pyi -> build/lib/tokenizers/decoders
  copying tokenizers/normalizers/__init__.pyi -> build/lib/tokenizers/normalizers
  copying tokenizers/pre_tokenizers/__init__.pyi -> build/lib/tokenizers/pre_tokenizers
  copying tokenizers/processors/__init__.pyi -> build/lib/tokenizers/processors
  copying tokenizers/trainers/__init__.pyi -> build/lib/tokenizers/trainers
  running build_ext
  running build_rust
  error: can't find Rust compiler
  
  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.
  
  To update pip, run:
  
      pip install --upgrade pip
  
  and then retry package installation.
  
  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.
  ----------------------------------------
  ERROR: Failed building wheel for tokenizers
  Building wheel for mosestokenizer (setup.py): started
  Building wheel for mosestokenizer (setup.py): finished with status 'done'
  Created wheel for mosestokenizer: filename=mosestokenizer-1.1.0-py3-none-any.whl size=49119 sha256=2f497e94001ba055eece56191ef57e0f8eb277d58177dd4ad4c6022bb3317044
  Stored in directory: /home/moisioa3/.cache/pip/wheels/fc/ce/48/78da8e3d51377fc5c4a178d5d02eada2aaea80c0ddb2b70865
  Building wheel for docopt (setup.py): started
  Building wheel for docopt (setup.py): finished with status 'done'
  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=fd12087983d74c133d31f5872a279855d8dd546da6bab0a97a95e29f687f382f
  Stored in directory: /home/moisioa3/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b
  Building wheel for toolwrapper (setup.py): started
  Building wheel for toolwrapper (setup.py): finished with status 'done'
  Created wheel for toolwrapper: filename=toolwrapper-2.1.0-py3-none-any.whl size=3353 sha256=61e52d96657f1de2caf3ef4b55f19d9ef26ec67de030d74c45f6a1518acd1d9f
  Stored in directory: /home/moisioa3/.cache/pip/wheels/7f/fb/ad/2b280cddd52c15c21ac9599c2661fe8aec93778fb35da77c7f
  Building wheel for uctools (setup.py): started
  Building wheel for uctools (setup.py): finished with status 'done'
  Created wheel for uctools: filename=uctools-1.3.0-py3-none-any.whl size=6161 sha256=e19d48859586a56bd2e6bc91bba57840590e8725e006f1dcf71cad71a245caa4
  Stored in directory: /home/moisioa3/.cache/pip/wheels/58/34/ff/68e3afc33b6758aeef228ea97955d8f79f125b91d9d45a09ae
Successfully built gluonnlp mosestokenizer docopt toolwrapper uctools
Failed to build tokenizers
ERROR: Could not build wheels for tokenizers which use PEP 517 and cannot be installed directly
Package    Version
---------- -------------------
certifi    2021.5.30
pip        21.1.3
setuptools 52.0.0.post20210125
wheel      0.36.2
Collecting mxnet-cu110
/var/spool/slurmd/job62015310/slurm_script: line 22: 118619 Killed                  pip install mxnet-cu110
Package    Version
---------- -------------------
certifi    2021.5.30
pip        21.1.3
setuptools 52.0.0.post20210125
wheel      0.36.2
slurmstepd: error: Detected 1 oom-kill event(s) in step 62015310.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
